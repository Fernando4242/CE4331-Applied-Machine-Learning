# -*- coding: utf-8 -*-
"""homework4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hpGJo2W7JVw9Cx78_Db6T-xy_NKaRrmw

# Initial Setup
"""

from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Load and split dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
assert x_train.shape == (60000, 28, 28)
assert x_test.shape == (10000, 28, 28)
assert y_train.shape == (60000,)
assert y_test.shape == (10000,)

# Normalize pixel values to be between 0 and 1
np.unique(y_train, return_counts=True)
x_train, x_test = x_train/255, x_test/255

"""# Model Definition and Training: Building, Compiling, and Fitting the Neural Network

The Flatten layer converts the 28x28 pixel image into a 1D array of 784 values so the model can process it. The first Dense layer with 256 neurons helps the model learn patterns from the input data using the ReLU activation function. The Dropout layer randomly disables 30% of neurons to prevent overfitting by making the model more general. The second Dense layer with 128 neurons further helps the model learn more complex patterns, again using ReLU. Another Dropout layer is added to keep the model from memorizing the data and overfitting. Finally, the Dense layer with 10 neurons outputs the probabilities for each digit (0-9), using the softmax function to decide the most likely digit to predict.
"""

# Define a neural network model with layers for flattening, dense, and dropout to prevent overfitting
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(256, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(10, activation='softmax')
])

# Compile the model with an optimizer, loss function, and metrics for evaluation
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Epochs allow the model to learn from the data over 20 iterations, improving its performance with each pass.
history = model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test))

"""# Learning Curves: Training and Test Accuracy vs Epoch

Removing memorization helps the model focus on learning general patterns instead of just remembering the training data. This way, it performs better on new, unseen data and improves its accuracy on test datasets.
"""

# Plot Training Accuracy and Validation Accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training vs Test Accuracy per Epoch')
plt.legend()
plt.grid(True)
plt.show()

# Plot the training and test loss over epochs to visualize the model's performance
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Test Loss per Epoch')
plt.legend()
plt.grid(True)
plt.show()